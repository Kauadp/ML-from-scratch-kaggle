# ğŸš€ Ensembles do Zero â€” Random Forest & Gradient Boosting no Kaggle (Sem Bibliotecas de ML)

![Python](https://img.shields.io/badge/Python-3.x-blue?logo=python)
![NumPy](https://img.shields.io/badge/NumPy-Linear%20Algebra-orange?logo=numpy)
![No ML Libraries](https://img.shields.io/badge/Machine%20Learning%20Libraries-None-red)
![Kaggle](https://img.shields.io/badge/Kaggle-LRMSE%200.068-20beff?logo=kaggle)
![Status](https://img.shields.io/badge/Status-Completed-success)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa **modelos de ensemble do zero**, especificamente:

- ğŸŒ³ **Random Forest para regressÃ£o**
- ğŸš€ **Gradient Boosting para regressÃ£o**

sem utilizar **nenhuma biblioteca de Machine Learning** (como scikit-learn, xgboost, lightgbm, etc.).

O objetivo principal foi **entender profundamente como algoritmos de ensemble funcionam internamente**, desde a construÃ§Ã£o das Ã¡rvores atÃ© o processo de otimizaÃ§Ã£o sequencial por gradiente.

Todo o pipeline foi implementado manualmente:

- PrÃ©-processamento e normalizaÃ§Ã£o dos dados  
- Ãrvores de regressÃ£o (CART simplificada)  
- Bootstrap sampling (bagging) para Random Forest  
- SeleÃ§Ã£o aleatÃ³ria de features (*m_try*)  
- CÃ¡lculo de resÃ­duos (gradiente da perda) para Boosting  
- Ensemble de modelos  
- Grid Search com validaÃ§Ã£o cruzada manual  
- GeraÃ§Ã£o de submissÃ£o para o Kaggle  

---

## ğŸ† Resultados no Kaggle

- âœ… **Public Score (LRMSE): 0.068**  
- âœ… **Private Score (LRMSE): 0.069**

A pequena diferenÃ§a entre os scores indica **boa capacidade de generalizaÃ§Ã£o** e baixo overfitting ao leaderboard pÃºblico.

---

## ğŸ“ˆ Resultados Visuais

### ğŸ“Š PrediÃ§Ãµes vs Valores Reais

O modelo final de Gradient Boosting conseguiu capturar muito bem a relaÃ§Ã£o entre os valores previstos e os valores reais:

![PrediÃ§Ãµes vs Reais](imgs/result.png)

---

### ğŸ… Score no Kaggle

Print do leaderboard da competiÃ§Ã£o:

![Score Kaggle](imgs/score.png)

---

## ğŸŒ³ Random Forest â€” ImplementaÃ§Ã£o do Zero

Foi implementada uma funÃ§Ã£o completa de **Random Forest para regressÃ£o**, contendo:

- Bootstrap sampling (amostragem com reposiÃ§Ã£o)
- Treinamento de mÃºltiplas Ã¡rvores independentes
- SeleÃ§Ã£o aleatÃ³ria de subconjuntos de variÃ¡veis em cada split (*m_try*)
- AgregaÃ§Ã£o das prediÃ§Ãµes por mÃ©dia

O Random Forest foi utilizado como **modelo base de comparaÃ§Ã£o** e tambÃ©m para guiar o espaÃ§o de busca dos hiperparÃ¢metros do Gradient Boosting.

### ğŸ” Grid Search com "Cross-Validation" Manual

Foi implementado um processo de **Grid Search manual**, com divisÃ£o treino/validaÃ§Ã£o, avaliando:

- `n_estimators`
- `min_frac` (fraÃ§Ã£o mÃ­nima de amostras por nÃ³)
- `m_try` (no caso do RF)
- `learning_rate` (no caso do GB)
- `max_depth`

Cada combinaÃ§Ã£o foi treinada e avaliada usando RMSE / LRMSE, de forma totalmente manual, sem bibliotecas prontas de validaÃ§Ã£o cruzada.

---

### ğŸ§® Modelo MatemÃ¡tico â€” Gradient Boosting (forma conceitual)

O Gradient Boosting constrÃ³i um **modelo aditivo**, onde cada nova Ã¡rvore corrige os erros das anteriores.

O modelo final pode ser representado conceitualmente como:

F(x) = Î½ * h1(x) + Î½ * h2(x) + ... + Î½ * hM(x)

onde:

- `h_m(x)` Ã© a m-Ã©sima Ã¡rvore de regressÃ£o  
- `Î½` (nu) Ã© a taxa de aprendizado (*learning rate*)  
- `M` Ã© o nÃºmero total de Ã¡rvores no ensemble  

---

Em cada iteraÃ§Ã£o, calcula-se o **resÃ­duo**, que corresponde ao gradiente negativo da funÃ§Ã£o de perda em relaÃ§Ã£o Ã s prediÃ§Ãµes atuais do modelo:

r_i = - dL(y_i, F(x_i)) / dF(x_i)

onde:

- `y_i` Ã© o valor real  
- `F(x_i)` Ã© a prediÃ§Ã£o atual do ensemble  
- `L` Ã© a funÃ§Ã£o de perda (neste projeto, RMSLE)

Cada nova Ã¡rvore Ã© treinada para prever esses resÃ­duos, e suas prediÃ§Ãµes sÃ£o adicionadas ao modelo existente, reduzindo progressivamente o erro.


---

## â±ï¸ Custo Computacional â€” Treinamento de 22 Horas

O treinamento final do modelo levou aproximadamente:

> â³ **1300 minutos (~22 horas)**

Rodando em:

- CPU: Intel i5 (9Âª geraÃ§Ã£o)
- RAM: 8 GB
- ImplementaÃ§Ã£o puramente em Python + NumPy

Isso mostra que:

- âœ”ï¸ O algoritmo Ã© **funcional e correto**
- âŒ NÃ£o Ã© computacionalmente viÃ¡vel para produÃ§Ã£o sem otimizaÃ§Ãµes em C/C++ (como nas libs do mercado)

Esse custo elevado vem principalmente de:

- Complexidade das Ã¡rvores
- NÃºmero de estimadores
- Tamanho do dataset (centenas de milhares de linhas)
- ImplementaÃ§Ã£o sem paralelizaÃ§Ã£o

Mesmo assim, o experimento prova que Ã© **possÃ­vel implementar modelos de alto desempenho do zero**, ainda que com custo computacional alto.

---

## ğŸ§  Conceitos Implementados

- AnÃ¡lise ExploratÃ³ria de Dados (EDA)
- PadronizaÃ§Ã£o de variÃ¡veis
- Ãrvores de RegressÃ£o (CART simplificada)
- Bagging (Bootstrap Aggregating)
- Random Forest
- Gradient Boosting
- FunÃ§Ã£o de perda RMSLE
- Gradiente da funÃ§Ã£o de perda
- Grid Search manual
- AvaliaÃ§Ã£o com conjunto de validaÃ§Ã£o
- ComparaÃ§Ã£o entre modelos

---

## âš™ï¸ Tecnologias Utilizadas

- Python  
- NumPy  
- Pandas  
- Matplotlib  

ğŸš« Nenhuma biblioteca de Machine Learning foi utilizada para treinamento.

---

## ğŸ¯ ConclusÃµes

- O Gradient Boosting superou o Random Forest apÃ³s ajuste de hiperparÃ¢metros.
- O ganho final veio principalmente do:
  - Aumento do nÃºmero de estimadores
  - Ajuste fino de profundidade das Ã¡rvores
  - Uso de learning rate menor
- Melhorias futuras viriam principalmente de:
  - Feature engineering
  - ImplementaÃ§Ã£o mais eficiente (Cython / Numba / C++)
  - Uso de validaÃ§Ã£o cruzada completa (k-fold)

O projeto teve foco **educacional e tÃ©cnico**, visando entender profundamente algoritmos de ensemble utilizados em competiÃ§Ãµes e no mercado.

---

## ğŸ‘¤ Autor

**KauÃ£ Dias**  
Estudante de EstatÃ­stica e entusiasta de CiÃªncia de Dados

- ğŸ™ GitHub: https://github.com/Kauadp  
- ğŸ”— LinkedIn: https://www.linkedin.com/in/kauad/

---

## ğŸ“š ReferÃªncias TeÃ³ricas

[1] Friedman, J. H. (2001).  
**Greedy Function Approximation: A Gradient Boosting Machine**

[2] Breiman, L. (2001).  
**Random Forests**

[3] Hastie, T.; Tibshirani, R.; Friedman, J.  
**The Elements of Statistical Learning (ESL)** â€” Springer.

[4] James, G.; Witten, D.; Hastie, T.; Tibshirani, R.  
**An Introduction to Statistical Learning (ISLR)** â€” Springer.

[5] Lay, D. C.; Lay, S. R.; McDonald, J. J.  
**Ãlgebra Linear e Suas AplicaÃ§Ãµes** â€” Pearson.

[6] Stewart, J.  
**CÃ¡lculo â€” Volumes 1 e 2** â€” Cengage Learning.
